{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "02e1dbad",
      "metadata": {
        "id": "02e1dbad",
        "outputId": "c702699a-1973-4c25-febb-6e4bdf63c021",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.235-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.235-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.235 ultralytics-thop-2.0.18\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "def in_colab() -> bool:\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if in_colab():\n",
        "    !pip install ultralytics\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5da71838",
      "metadata": {
        "id": "5da71838"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "from torchvision import transforms, datasets\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import platform\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from ultralytics import YOLO\n",
        "import shutil\n",
        "import cv2\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c4acce",
      "metadata": {
        "id": "73c4acce",
        "outputId": "7b6656d0-b0fa-44f0-b30e-928a175e8b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device.type}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c780d112",
      "metadata": {
        "id": "c780d112"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c13042",
      "metadata": {
        "id": "10c13042",
        "outputId": "ad9acf37-ba83-4324-d6d7-cad9a9030d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total images with incorrect size: 5249\n",
            "Total non-square images: 697\n",
            "Total non-RGB images: 1002\n"
          ]
        }
      ],
      "source": [
        "# Check the sizes of the images and if any are in RGBA format\n",
        "if in_colab():\n",
        "    root = '/content/drive/MyDrive/2025-2026/Semester 1/MTH 4320/Project 2/brain-mri'\n",
        "else:\n",
        "    root = 'brain-mri'\n",
        "splits = [dir for dir in os.listdir(root) if dir != '.DS_Store']\n",
        "categories = [dir for dir in os.listdir(os.path.join(root, splits[0])) if dir != '.DS_Store']\n",
        "size = 128\n",
        "img_size = (size, size)\n",
        "count = 0\n",
        "non_square = 0\n",
        "non_RGB_count = 0\n",
        "for split in splits:\n",
        "    for category in categories:\n",
        "        path = os.path.join(root, split, category, 'images')\n",
        "        for img_name in os.listdir(path):\n",
        "            if not img_name.endswith('.jpg'):\n",
        "                continue\n",
        "            img_path = os.path.join(path, img_name)\n",
        "            img = Image.open(img_path)\n",
        "            if img.mode != 'RGB':\n",
        "                non_RGB_count += 1\n",
        "            if img.size != img_size:\n",
        "                #print(f\"Image {img_path} has size {img.size}\")\n",
        "                if img.size[0] != img.size[1]:\n",
        "                    #print(f\"Image {img_path} is not square: {img.size}\")\n",
        "                    non_square += 1\n",
        "                count += 1\n",
        "print(f\"Total images with incorrect size: {count}\")\n",
        "print(f\"Total non-square images: {non_square}\")\n",
        "print(f\"Total non-RGB images: {non_RGB_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f515879",
      "metadata": {
        "id": "2f515879"
      },
      "outputs": [],
      "source": [
        "# Resize images to 128x128 and adjust labels for non-square images\n",
        "resized_dir = 'resized-data'\n",
        "shutil.rmtree(resized_dir, ignore_errors=True)\n",
        "os.makedirs(resized_dir, exist_ok=True)\n",
        "\n",
        "for split in splits:\n",
        "    # Make the new directories\n",
        "    for category in categories:\n",
        "        resized_img_path = os.path.join(resized_dir, split, category, 'images')\n",
        "        resized_label_path = os.path.join(resized_dir, split, category, 'labels')\n",
        "        os.makedirs(resized_img_path, exist_ok=True)\n",
        "        os.makedirs(resized_label_path, exist_ok=True)\n",
        "\n",
        "# Resize images and labels\n",
        "for split in splits:\n",
        "    for category in categories:\n",
        "        img_path = os.path.join(root, split, category, 'images')\n",
        "        label_path = os.path.join(root, split, category, 'labels')\n",
        "        for img_name in os.listdir(img_path):\n",
        "            # Skip .DS_Store and whatever else is not a .jpg image\n",
        "            if not img_name.endswith('.jpg'):\n",
        "                continue\n",
        "\n",
        "            # Skip images with no labels\n",
        "            label_file = img_name.replace('.jpg', '.txt')\n",
        "            if not os.path.exists(os.path.join(label_path, label_file)):\n",
        "                continue\n",
        "\n",
        "            # Skip if there is an image with no labels\n",
        "            if not os.path.exists(os.path.join(label_path, label_file)):\n",
        "                continue\n",
        "\n",
        "            # Skip 128x128 images\n",
        "            img_full_path = os.path.join(img_path, img_name)\n",
        "            img = Image.open(img_full_path)\n",
        "\n",
        "            # Convert RGBA to RGB if necessary\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')\n",
        "\n",
        "            if img.size == img_size:\n",
        "                # Copy the image and label as is\n",
        "                shutil.copy(img_full_path, os.path.join(resized_dir, split, category, 'images', img_name))\n",
        "                label_full_path = os.path.join(label_path, img_name.replace('.jpg', '.txt'))\n",
        "                shutil.copy(label_full_path, os.path.join(resized_dir, split, category, 'labels', img_name.replace('.jpg', '.txt')))\n",
        "\n",
        "            # Resize non-128x128 square images\n",
        "            elif img.size[0] == img.size[1]:\n",
        "                # Resize the image and copy to new dir\n",
        "                img_resized = img.resize(img_size)\n",
        "                img_resized.save(os.path.join(resized_dir, split, category, 'images', img_name))\n",
        "                # Copy the label as is\n",
        "                label_full_path = os.path.join(label_path, img_name.replace('.jpg', '.txt'))\n",
        "                shutil.copy(label_full_path, os.path.join(resized_dir, split, category, 'labels', img_name.replace('.jpg', '.txt')))\n",
        "\n",
        "            # Resize non-square images and adjust labels\n",
        "            else:\n",
        "                original_width, original_height = img.size\n",
        "                img_resized = img.resize(img_size)\n",
        "                img_resized.save(os.path.join(resized_dir, split, category, 'images', img_name))\n",
        "\n",
        "                # Adjust the label\n",
        "                label_full_path = os.path.join(label_path, img_name.replace('.jpg', '.txt'))\n",
        "                with open(label_full_path, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "                adjusted_lines = []\n",
        "                for line in lines:\n",
        "                    components = line.strip().split()\n",
        "                    class_id = components[0]\n",
        "                    x_center = float(components[1])\n",
        "                    y_center = float(components[2])\n",
        "                    width = float(components[3])\n",
        "                    height = float(components[4])\n",
        "\n",
        "                    # Adjust x_center and width\n",
        "                    x_center_adj = x_center * (original_width / size)\n",
        "                    width_adj = width * (original_width / size)\n",
        "\n",
        "                    # Adjust y_center and height\n",
        "                    y_center_adj = y_center * (original_height / size)\n",
        "                    height_adj = height * (original_height / size)\n",
        "                    adjusted_line = f\"{class_id} {x_center_adj:.6f} {y_center_adj:.6f} {width_adj:.6f} {height_adj:.6f}\\n\"\n",
        "                    adjusted_lines.append(adjusted_line)\n",
        "\n",
        "                # Write the adjusted label\n",
        "                with open(os.path.join(resized_dir, split, category, 'labels', img_name.replace('.jpg', '.txt')), 'w') as f:\n",
        "                    f.writelines(adjusted_lines)\n",
        "\n",
        "# 11 seconds on laptop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3073dec",
      "metadata": {
        "id": "e3073dec"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class\n",
        "class BrainMRIDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        # Load all splits and categories\n",
        "        for split in os.listdir(root_dir):\n",
        "            if split not in splits:\n",
        "                continue\n",
        "            split_dir = os.path.join(root_dir, split)\n",
        "            for category in os.listdir(split_dir):\n",
        "                if category not in categories:\n",
        "                    continue\n",
        "                img_dir = os.path.join(split_dir, category, 'images')\n",
        "                label_dir = os.path.join(split_dir, category, 'labels')\n",
        "\n",
        "                for img_name in os.listdir(img_dir):\n",
        "                    if img_name.endswith('.jpg'):\n",
        "                        self.samples.append({\n",
        "                            'img_path': os.path.join(img_dir, img_name),\n",
        "                            'label_path': os.path.join(label_dir, img_name.replace('.jpg', '.txt'))\n",
        "                        })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(sample['img_path']).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Load labels\n",
        "        boxes = []\n",
        "        with open(sample['label_path'], 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                components = line.strip().split()\n",
        "                class_id = int(components[0])\n",
        "                x_center = float(components[1])\n",
        "                y_center = float(components[2])\n",
        "                width = float(components[3])\n",
        "                height = float(components[4])\n",
        "                boxes.append([class_id, x_center, y_center, width, height])\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "\n",
        "        return image, boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c02a6d0b",
      "metadata": {
        "id": "c02a6d0b",
        "outputId": "65d6e1dd-eebe-4db6-a537-3b9f8890bc8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 4197\n",
            "Val size: 526\n",
            "Test size: 524\n"
          ]
        }
      ],
      "source": [
        "# Dataloader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = BrainMRIDataset(resized_dir, transform=transform)\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = int(0.1 * len(dataset))\n",
        "val_size = len(dataset) - train_size - test_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Val size: {len(val_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f07d2bba",
      "metadata": {
        "id": "f07d2bba"
      },
      "outputs": [],
      "source": [
        "# Collate function to handle YOLO labels\n",
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    labels = []\n",
        "    boxes = []\n",
        "\n",
        "    for image, box in batch:\n",
        "        images.append(image)\n",
        "        labels.append(int(box[0][0]))\n",
        "        boxes.append(box[0][1:5])\n",
        "\n",
        "    images = torch.stack(images)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    boxes = torch.stack(boxes)\n",
        "\n",
        "    return images, labels, boxes\n",
        "\n",
        "# Dataloaders with collate\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ac2f6c3",
      "metadata": {
        "id": "7ac2f6c3"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6337597",
      "metadata": {
        "id": "e6337597"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8449689a",
      "metadata": {
        "id": "8449689a"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 16 * 16, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        # Bounding box head\n",
        "        self.box_regressor = nn.Sequential(\n",
        "            nn.Linear(256 * 16 * 16, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 4),\n",
        "            nn.Sigmoid()  # Squish to [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        class_pred = self.classifier(x)\n",
        "        box_pred = self.box_regressor(x)\n",
        "        return class_pred, box_pred\n",
        "\n",
        "model = CNN().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dc275b5",
      "metadata": {
        "id": "1dc275b5"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a21fd0",
      "metadata": {
        "id": "95a21fd0"
      },
      "outputs": [],
      "source": [
        "# Define the training parameters\n",
        "model = CNN(num_classes=4).to(device)\n",
        "class_criterion = nn.CrossEntropyLoss()\n",
        "box_criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 100\n",
        "history = {\n",
        "    \"train_loss\": [], \"train_class_loss\": [], \"train_box_loss\": [], \"train_acc\": [],\n",
        "    \"val_loss\": [], \"val_class_loss\": [], \"val_box_loss\": [], \"val_acc\": []\n",
        "}\n",
        "\n",
        "# Calculate accuracy from logits\n",
        "def accuracy_from_logits(logits, y):\n",
        "    preds = logits.argmax(1)  # choose class with highest predicted score\n",
        "    return (preds == y).float().mean().item()  # fraction of correct predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8977c0",
      "metadata": {
        "id": "7f8977c0"
      },
      "outputs": [],
      "source": [
        "def train(name):\n",
        "    # Early stopping settings\n",
        "    patience = 10          # epochs to wait after last improvement\n",
        "    min_delta = 0.0        # minimum change in val_loss to qualify as improvement\n",
        "    best_val = 10e20       # track best validation loss\n",
        "    best_epoch = -1\n",
        "    patience_ctr = 0\n",
        "    best_ckpt_path = \"best.pt\"\n",
        "\n",
        "    train_start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss, running_class_loss, running_box_loss = 0.0, 0.0, 0.0\n",
        "        running_correct, total = 0, 0\n",
        "\n",
        "        for x, class_labels, boxes in train_loader:\n",
        "            x = x.to(device)\n",
        "            class_labels = class_labels.to(device)\n",
        "            boxes = boxes.to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            class_pred, box_pred = model(x)\n",
        "\n",
        "            # Combined loss\n",
        "            loss_class = class_criterion(class_pred, class_labels)\n",
        "            loss_box = box_criterion(box_pred, boxes)\n",
        "            loss = loss_class + 5.0 * loss_box  # Weight box loss more\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * x.size(0)\n",
        "            running_class_loss += loss_class.item() * x.size(0)\n",
        "            running_box_loss += loss_box.item() * x.size(0)\n",
        "            running_correct += (class_pred.argmax(1) == class_labels).sum().item()\n",
        "            total += x.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_class_loss = running_class_loss / total\n",
        "        train_box_loss = running_box_loss / total\n",
        "        train_acc = running_correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_running_loss, val_running_class_loss, val_running_box_loss = 0.0, 0.0, 0.0\n",
        "        val_running_correct, val_total = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, class_labels, boxes in val_loader:\n",
        "                x = x.to(device)\n",
        "                class_labels = class_labels.to(device)\n",
        "                boxes = boxes.to(device)\n",
        "\n",
        "                class_pred, box_pred = model(x)\n",
        "\n",
        "                loss_class = class_criterion(class_pred, class_labels)\n",
        "                loss_box = box_criterion(box_pred, boxes)\n",
        "                loss = loss_class + 5.0 * loss_box\n",
        "\n",
        "                val_running_loss += loss.item() * x.size(0)\n",
        "                val_running_class_loss += loss_class.item() * x.size(0)\n",
        "                val_running_box_loss += loss_box.item() * x.size(0)\n",
        "                val_running_correct += (class_pred.argmax(1) == class_labels).sum().item()\n",
        "                val_total += x.size(0)\n",
        "\n",
        "        val_loss = val_running_loss / val_total\n",
        "        val_class_loss = val_running_class_loss / val_total\n",
        "        val_box_loss = val_running_box_loss / val_total\n",
        "        val_acc = val_running_correct / val_total\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_class_loss\"].append(train_class_loss)\n",
        "        history[\"train_box_loss\"].append(train_box_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_class_loss\"].append(val_class_loss)\n",
        "        history[\"val_box_loss\"].append(val_box_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        print(f\"Epoch {epoch:02d} | \"\n",
        "            f\"train: loss={train_loss:.4f} (cls={train_class_loss:.4f}, box={train_box_loss:.4f}), acc={train_acc:.4f} | \"\n",
        "            f\"val: loss={val_loss:.4f} (cls={val_class_loss:.4f}, box={val_box_loss:.4f}), acc={val_acc:.4f} | \"\n",
        "            f\"time: {epoch_time:.2f}s\")\n",
        "\n",
        "        # Early stopping check (monitor val_loss)\n",
        "        if val_loss < best_val - min_delta:\n",
        "            best_val = val_loss\n",
        "            best_epoch = epoch\n",
        "            patience_ctr = 0\n",
        "            # Save best checkpoint so far\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(),\n",
        "                \"history\": history,\n",
        "                \"best_val_loss\": best_val\n",
        "            }, best_ckpt_path)\n",
        "            print(f\"  -> New best val_loss {best_val:.4f} at epoch {epoch}. Saved to {best_ckpt_path}.\")\n",
        "        else:\n",
        "            patience_ctr += 1\n",
        "            if patience_ctr >= patience:\n",
        "                print(f\"\\nEarly stopping triggered at epoch {epoch} \"\n",
        "                    f\"(no improvement for {patience} epochs). Best epoch: {best_epoch}.\")\n",
        "                break\n",
        "\n",
        "    # Total training time\n",
        "    total_time = time.time() - train_start_time\n",
        "    print(f\"\\nTotal training time: {total_time:.2f}s\")\n",
        "    print(f\"Best epoch: {best_epoch} | Best val_loss: {best_val:.4f}\")\n",
        "\n",
        "    # Restore best model before final save (in case we stopped after it)\n",
        "    ckpt = torch.load(best_ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "\n",
        "    # Save final artifact (model + history)\n",
        "    final_path = f\"models/{name}.pth\"\n",
        "    torch.save({\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"history\": history,\n",
        "        \"best_epoch\": best_epoch,\n",
        "        \"best_val_loss\": best_val\n",
        "    }, final_path)\n",
        "    print(f\"Final model saved to '{final_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bdd0569",
      "metadata": {
        "id": "2bdd0569"
      },
      "outputs": [],
      "source": [
        "def display_results(path):\n",
        "    # Load the trained model from file\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state\"])\n",
        "    history = checkpoint[\"history\"]\n",
        "\n",
        "    model.eval()  # set to evaluation mode\n",
        "\n",
        "    # Prepare x-axis for plots\n",
        "    epochs_range = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    # Create figure with 3 subplots\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Total Loss\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Total Loss\")\n",
        "    ax1.plot(epochs_range, history[\"train_loss\"], label=\"Train Loss\", linestyle=\"-\")\n",
        "    ax1.plot(epochs_range, history[\"val_loss\"], label=\"Val Loss\", linestyle=\"--\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(\"Total Loss over Epochs\")\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Classification and box Loss\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax2.set_ylabel(\"Loss\")\n",
        "    ax2.plot(epochs_range, history[\"train_class_loss\"], label=\"Train Class Loss\", linestyle=\"-\", color=\"tab:blue\")\n",
        "    ax2.plot(epochs_range, history[\"val_class_loss\"], label=\"Val Class Loss\", linestyle=\"--\", color=\"tab:blue\")\n",
        "    ax2.plot(epochs_range, history[\"train_box_loss\"], label=\"Train box Loss\", linestyle=\"-\", color=\"tab:orange\")\n",
        "    ax2.plot(epochs_range, history[\"val_box_loss\"], label=\"Val box Loss\", linestyle=\"--\", color=\"tab:orange\")\n",
        "    ax2.legend()\n",
        "    ax2.set_title(\"Classification & box Loss over Epochs\")\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy\n",
        "    ax3.set_xlabel(\"Epoch\")\n",
        "    ax3.set_ylabel(\"Accuracy\")\n",
        "    ax3.plot(epochs_range, history[\"train_acc\"], label=\"Train Acc\", linestyle=\"-\")\n",
        "    ax3.plot(epochs_range, history[\"val_acc\"], label=\"Val Acc\", linestyle=\"--\")\n",
        "    ax3.legend()\n",
        "    ax3.set_title(\"Accuracy over Epochs\")\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Confusion matrix and bounding box evaluation\n",
        "    # Variables to track accuracy\n",
        "    test_correct, test_total = 0, 0\n",
        "    total_iou = 0.0\n",
        "\n",
        "    # Lists to store predictions and true labels for the confusion matrix\n",
        "    all_preds, all_trues = [], []\n",
        "\n",
        "    # Disable gradient computation for faster evaluation\n",
        "    with torch.no_grad():\n",
        "        # Loop through the test set in batches\n",
        "        for images, class_labels, boxes in test_loader:\n",
        "            # Move inputs to the computation device\n",
        "            images = images.to(device)\n",
        "            class_labels = class_labels.to(device)\n",
        "            boxes = boxes.to(device)\n",
        "\n",
        "            # Forward pass to get model outputs\n",
        "            class_pred, box_pred = model(images)\n",
        "\n",
        "            # Get predicted class indices (highest logit per sample)\n",
        "            preds = class_pred.argmax(1).cpu().numpy()\n",
        "\n",
        "            # Store predictions and ground truth labels\n",
        "            all_preds.append(preds)\n",
        "            all_trues.append(class_labels.cpu().numpy())\n",
        "\n",
        "            # Update accuracy counters\n",
        "            test_correct += (preds == class_labels.cpu().numpy()).sum()\n",
        "            test_total += class_labels.size(0)\n",
        "\n",
        "            # Calculate IoU for bounding boxes\n",
        "            # Convert from (x_center, y_center, w, h) to (x1, y1, x2, y2)\n",
        "            def xywh_to_xyxy(boxes):\n",
        "                x_center, y_center, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
        "                x1 = x_center - w / 2\n",
        "                y1 = y_center - h / 2\n",
        "                x2 = x_center + w / 2\n",
        "                y2 = y_center + h / 2\n",
        "                return torch.stack([x1, y1, x2, y2], dim=1)\n",
        "\n",
        "            pred_boxes_xyxy = xywh_to_xyxy(box_pred)\n",
        "            true_boxes_xyxy = xywh_to_xyxy(boxes)\n",
        "\n",
        "            # Calculate IoU\n",
        "            from torchvision.ops import box_iou\n",
        "            ious = box_iou(pred_boxes_xyxy, true_boxes_xyxy)\n",
        "            total_iou += ious.diag().sum().item()\n",
        "\n",
        "    # Compute overall test accuracy\n",
        "    test_acc = test_correct / test_total\n",
        "    avg_iou = total_iou / test_total\n",
        "    print(f\"Test Classification Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Average IoU (Bounding Box): {avg_iou:.4f}\")\n",
        "\n",
        "    # Flatten predictions and true labels into 1D arrays\n",
        "    y_true = np.concatenate(all_trues).ravel()\n",
        "    y_pred = np.concatenate(all_preds).ravel()\n",
        "\n",
        "    # Detect classes from both true and predicted\n",
        "    class_labels_unique = np.unique(np.concatenate((y_true, y_pred)))\n",
        "    class_names = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
        "    display_names = [class_names[i] if i < len(class_names) else str(i) for i in class_labels_unique]\n",
        "    print(f\"Detected {len(class_labels_unique)} classes: {class_labels_unique}\")\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=class_labels_unique)\n",
        "\n",
        "    # Plot annotated confusion matrix\n",
        "    plt.figure(figsize=(8, 7))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,              # annotate all cells\n",
        "        fmt=\"d\",                 # integer format\n",
        "        cbar=True,\n",
        "        xticklabels=display_names,\n",
        "        yticklabels=display_names,\n",
        "        cmap=\"Blues\"\n",
        "    )\n",
        "    plt.title(f\"Confusion Matrix (Test)\\nAccuracy: {test_acc:.4f}, Avg IoU: {avg_iou:.4f}\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bba6452",
      "metadata": {
        "id": "8bba6452",
        "outputId": "2e7444ef-d8fe-47ab-9e2f-f3bef10162b1"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCNN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m display_results(\u001b[33m\"\u001b[39m\u001b[33mmodels/CNN.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     33\u001b[39m loss.backward()\n\u001b[32m     34\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * x.size(\u001b[32m0\u001b[39m)\n\u001b[32m     37\u001b[39m running_class_loss += loss_class.item() * x.size(\u001b[32m0\u001b[39m)\n\u001b[32m     38\u001b[39m running_box_loss += loss_box.item() * x.size(\u001b[32m0\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "train(CNN)\n",
        "display_results(\"models/CNN.pth\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}